# -*- coding: utf-8 -*-
"""CRMAB Modelling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G4rtUazqFjhfU-F8ynBFmNY73ufi2StI

### ***Cell A: Sort, action mapping, imputation, computing weights***
"""

import pandas as pd, numpy as np, json, pickle, os
from pathlib import Path
OUT_DIR = Path("/content/crmab_outputs")
OUT_DIR.mkdir(parents=True, exist_ok=True)


DATA_PATH = Path("/content/final_traj_clean_training_heart_v2.csv")
FEAT_JSON = Path("/content/feature_config.json")
ORIG_WEIGHT_JSON = Path("/content/action_class_weights.json")

assert DATA_PATH.exists(), f"Missing dataset: {DATA_PATH}"
assert FEAT_JSON.exists(), f"Missing feature_config.json: {FEAT_JSON}"

df = pd.read_csv(DATA_PATH, low_memory=False)
with open(FEAT_JSON) as f:
    feat_cfg = json.load(f)

print("Rows:", len(df))
print("Unique hadm_id:", df['hadm_id'].nunique())
print("Min/Max timestep:", df['timestep'].min(), df['timestep'].max())
print("\nOriginal action distribution:\n", df['action'].value_counts().sort_index())


df = df.sort_values(['hadm_id','aug_id','timestep']).reset_index(drop=True)


bad = df.groupby(['hadm_id','aug_id'])['timestep'].apply(lambda x: not x.is_monotonic_increasing)
bad_count = int(bad.sum())
print("Groups with non-monotonic timesteps (hadm_id,aug_id):", bad_count)


if bad_count:
    sample_bad = list(bad[bad].index[:6])
    print("Sample bad groups (hadm_id,aug_id):", sample_bad)
    for h,a in sample_bad:
        print(f"\n=== Example group (hadm={h}, aug={a}) ===")
        display(df[(df['hadm_id']==h) & (df['aug_id']==a)].head(12))


dup_mask = df.duplicated(subset=['hadm_id','aug_id','timestep'], keep=False)
dup_count = int(dup_mask.sum())
print("Duplicates by (hadm,aug,timestep):", dup_count)
if dup_count:
    display(df[dup_mask].head(10))




state_cols = []
for g in ['vitals','labs','recency','flags']:
    if g in feat_cfg:
        state_cols += feat_cfg[g]
state_cols = [c for c in state_cols if c in df.columns]
print("Using state columns (d={}): {}".format(len(state_cols), state_cols))

# Map actions to compact labels 0..K-1 (important for LightGBM num_class)
unique_actions = sorted(df['action'].unique().tolist())
action_map = {old:i for i,old in enumerate(unique_actions)}
inv_action_map = {v:k for k,v in action_map.items()}
df['action_mapped'] = df['action'].map(action_map).astype(int)
print("Action mapping:", action_map)

# Recompute class weights (inverse frequency normalized to mean=1)
action_counts = df['action'].value_counts().to_dict()
inv = {int(k): 1.0 / v for k,v in action_counts.items()}
mean_inv = np.mean(list(inv.values()))
class_weights_recomputed = {int(k): float(v/mean_inv) for k,v in inv.items()}
print("Recomputed class weights (original labels):", class_weights_recomputed)

# Saving
(OUT_DIR / "action_label_map.json").write_text(json.dumps(action_map, indent=2))
(OUT_DIR / "action_label_map_inv.json").write_text(json.dumps({str(k):v for k,v in inv_action_map.items()}, indent=2))
(OUT_DIR / "action_class_weights_recomputed.json").write_text(json.dumps({str(k):v for k,v in class_weights_recomputed.items()}, indent=2))
print("Saved mapping + recomputed weights to", OUT_DIR)

# Imputation: forward/backfill per hadm_id then median fill (keeps imputation flags present)
cols_to_impute = state_cols.copy()
print("Imputation columns count:", len(cols_to_impute))
if len(cols_to_impute):
    df[cols_to_impute] = df.groupby('hadm_id')[cols_to_impute].ffill().bfill()
    for c in cols_to_impute:
        if df[c].isna().any():
            med = df[c].median()
            df[c] = df[c].fillna(med)

print("\nAfter imputation NA% (top):")
display(df[cols_to_impute].isna().mean().sort_values(ascending=False).head(25))

# Quick summary
print("Final rows:", len(df))
print("Unique hadm_id:", df['hadm_id'].nunique())
print("Action (mapped) distribution:", df['action_mapped'].value_counts().sort_index().to_dict())

# Saving cleaned dataframe to OUT_DIR
CLEAN_CSV = OUT_DIR / "final_traj_clean_training_heart_v2_sorted_imputed.csv"
df.to_csv(CLEAN_CSV, index=False)
print("Saved cleaned CSV:", CLEAN_CSV)

"""###***Cell B: Train behavior model p_clinician(a|s) and save model + propensities***üß≠"""

!pip install -q lightgbm
import lightgbm as lgb
from sklearn.model_selection import GroupKFold
from sklearn.metrics import classification_report, f1_score
import joblib, numpy as np, json
from pathlib import Path

OUT_DIR = Path("/content/crmab_outputs")



target_col = 'action_mapped'
core = feat_cfg.get('core', [])


if 'all_model_columns_ordered' in feat_cfg:
    feature_cols = [c for c in feat_cfg['all_model_columns_ordered'] if c not in core + [feat_cfg.get('target','action')]]
else:
    feature_cols = state_cols.copy()
feature_cols = [c for c in feature_cols if c in df.columns]
print("Behavior model features ({}): {}".format(len(feature_cols), feature_cols[:10]))

X = df[feature_cols].apply(pd.to_numeric, errors='coerce').fillna(0.0)
y = df[target_col].astype(int).values
groups = df['hadm_id'].astype(str).values


orig_weights = json.loads((OUT_DIR / "action_class_weights_recomputed.json").read_text())
orig_weights = {int(k): float(v) for k,v in orig_weights.items()}

class_weights_mapped = {}
for orig_label, w in orig_weights.items():
    if orig_label in action_map:
        class_weights_mapped[action_map[orig_label]] = float(w)

for m in range(len(set(action_map.values()))):
    class_weights_mapped.setdefault(m, 1.0)

print("Class weights mapped:", class_weights_mapped)

sample_w = np.array([class_weights_mapped[int(a)] for a in y], dtype=float)

num_classes = len(unique_actions)
params = {
    "objective":"multiclass", "num_class": int(num_classes),
    "learning_rate": 0.08, "n_estimators": 350, "num_leaves": 63,
    "subsample":0.9, "colsample_bytree":0.8, "random_state":42, "n_jobs":-1
}

gkf = GroupKFold(n_splits=5)
fold = 0
cv_f1s = []
for tr_idx, te_idx in gkf.split(X, y, groups):
    fold += 1
    X_tr, X_te = X.iloc[tr_idx], X.iloc[te_idx]
    y_tr, y_te = y[tr_idx], y[te_idx]
    w_tr = sample_w[tr_idx]
    clf = lgb.LGBMClassifier(**params)
    clf.fit(X_tr, y_tr, sample_weight=w_tr)
    preds = clf.predict(X_te)
    f1 = f1_score(y_te, preds, average='macro')
    cv_f1s.append(f1)
    print(f"[Fold {fold}] macro-F1: {f1:.4f}")

print("CV macro-F1 mean ¬± sd:", np.mean(cv_f1s), np.std(cv_f1s))

# Retraining on full data and save (original feature set)
final_clf = lgb.LGBMClassifier(**params)
final_clf.fit(X, y, sample_weight=sample_w)
joblib.dump(final_clf, OUT_DIR / "behavior_model_lgbm.pkl")
print("Saved behavior model ->", OUT_DIR / "behavior_model_lgbm.pkl")

# Saving predicted propensities (aligned with mapped labels 0..K-1)
probs = final_clf.predict_proba(X)
np.save(OUT_DIR / "behavior_propensity_probs.npy", probs)
print("Saved behavior propensities ->", OUT_DIR / "behavior_propensity_probs.npy")

"""###***Cell C: Build RMAB sequences pickle (mapped actions + computed reward)***"""

import pickle, numpy as np
from pathlib import Path
OUT_DIR = Path("/content/crmab_outputs")


def compute_reward_row(row):

    if 'reward' in row.index and not pd.isna(row['reward']):
        return float(row['reward'])

    si = row.get('si', np.nan)
    r_phys = 0.0
    if not pd.isna(si):
        r_phys += max(0.0, 1.5 - si)

    try:
        act_orig = int(row.get('action', 0))
    except:
        act_orig = 0
    action_cost = {0:0.0, 1:-0.05, 2:-0.05, 3:-0.1, 4:-0.2}.get(act_orig, -0.05)
    return float(r_phys + action_cost)


seqs = {}
id_cols = ['hadm_id','timestep','aug_id','action','action_mapped']
grouped = df.groupby(['hadm_id','aug_id'], sort=False)
for (hid, aug), g in grouped:
    g = g.sort_values('timestep')
    S = g[state_cols].to_numpy(dtype=float)
    A_mapped = g['action_mapped'].astype(int).to_numpy()

    if 'action' in g.columns:
        try:
            A_orig = g['action'].astype(int).to_numpy()
        except:
            A_orig = A_mapped.copy()
    else:
        A_orig = A_mapped.copy()
    T_arr = g['timestep'].astype(int).to_numpy()

    if 'reward' in g.columns:
        R = g['reward'].astype(float).to_numpy()
    else:
        R = g.apply(compute_reward_row, axis=1).to_numpy(dtype=float)
    seqs[(int(hid), int(aug))] = {'t': T_arr, 'S': S, 'A': A_mapped, 'A_orig': A_orig, 'R': R}

RMAB_PKL = OUT_DIR / "rmab_sequences.pkl"
with open(RMAB_PKL, "wb") as f:
    pickle.dump({"state_cols": state_cols, "sequences": seqs}, f, protocol=pickle.HIGHEST_PROTOCOL)
print("Saved RMAB sequences ->", RMAB_PKL, " num keys:", len(seqs))

# BLOCK 1: Retrain behavior model on state_cols
import json, joblib, numpy as np, pandas as pd
from pathlib import Path
import lightgbm as lgb

OUT_DIR = Path("/content/crmab_outputs")
OUT_DIR.mkdir(parents=True, exist_ok=True)


try:
    df
except NameError:
    df = pd.read_csv("/content/final_traj_clean_training_heart_v2_sorted_imputed.csv", low_memory=False)
try:
    state_cols
except NameError:
    FEAT_JSON = Path("/content/feature_config.json")
    feat_cfg = json.loads(FEAT_JSON.read_text())
    state_cols = []
    for g in ['vitals','labs','recency','flags']:
        if g in feat_cfg:
            state_cols += feat_cfg[g]
    state_cols = [c for c in state_cols if c in df.columns]

try:
    action_map
except NameError:
    unique_actions = sorted(df['action'].unique().tolist())
    action_map = {old: i for i, old in enumerate(unique_actions)}

# loading recomputed original-label keyed weights (if exists), else build simple inverse-frequency
orig_weights_path = OUT_DIR / "action_class_weights_recomputed.json"
if orig_weights_path.exists():
    orig_weights = json.loads(orig_weights_path.read_text())
    orig_weights = {int(k): float(v) for k, v in orig_weights.items()}
else:
    # fallback: inverse frequency normalized to mean=1 (original labels)
    action_counts = df['action'].value_counts().to_dict()
    inv = {int(k): 1.0 / v for k, v in action_counts.items()}
    mean_inv = np.mean(list(inv.values()))
    orig_weights = {int(k): float(v/mean_inv) for k, v in inv.items()}

# map orig label weights to mapped labels (safeguard missing keys)
class_weights_mapped = {}
for orig_label, w in orig_weights.items():
    if orig_label in action_map:
        class_weights_mapped[action_map[orig_label]] = float(w)
# ensure all mapped labels have a weight
num_mapped = len(set(action_map.values()))
for m in range(num_mapped):
    class_weights_mapped.setdefault(m, 1.0)

# Prepare training data (state_cols)
X_state = df[state_cols].apply(pd.to_numeric, errors='coerce').fillna(0.0)
y = df['action_mapped'].astype(int).values
sample_w = np.array([class_weights_mapped[int(a)] for a in y], dtype=float)

print(f"Retraining behavior model: n_samples={X_state.shape[0]}, n_features={X_state.shape[1]}, n_classes={len(class_weights_mapped)}")

params = {
    "objective": "multiclass",
    "num_class": int(len(class_weights_mapped)),
    "learning_rate": 0.08,
    "n_estimators": 350,
    "num_leaves": 63,
    "subsample": 0.9,
    "colsample_bytree": 0.8,
    "random_state": 42,
    "n_jobs": -1
}

clf = lgb.LGBMClassifier(**params)
clf.fit(X_state, y, sample_weight=sample_w)
model_path = OUT_DIR / "behavior_model_lgbm_statecols.pkl"
joblib.dump(clf, model_path)
print("Saved new behavior model ->", model_path)

# Save propensities aligned with state_cols
probs = clf.predict_proba(X_state)
np.save(OUT_DIR / "behavior_propensity_probs_statecols.npy", probs)
print("Saved propensities ->", OUT_DIR / "behavior_propensity_probs_statecols.npy")


print("behavior_model.classes_ (length):", getattr(clf, "classes_", None).shape if hasattr(clf, "classes_") else None)
print("Sample propensities shape:", probs.shape)

# BLOCK 2: Safer behavior predict_proba wrapper
import joblib, pandas as pd
from pathlib import Path

OUT_DIR = Path("/content/crmab_outputs")
behavior_model_path = OUT_DIR / "behavior_model_lgbm_statecols.pkl"
assert behavior_model_path.exists(), f"Behavior model not found at {behavior_model_path}"
behavior_model = joblib.load(behavior_model_path)

# Wrapper: accepts numpy array S_array matching state_cols order and returns shape (n, K)
def b_probs_from_state_array(S_array, state_cols_local=state_cols):
    """
    S_array: numpy array (n, d) in the exact order of state_cols_local
    returns: numpy array (n, K) of probabilities
    """
    import numpy as np
    # ensure 2D
    S_np = np.asarray(S_array)
    if S_np.ndim == 1:
        S_np = S_np.reshape(1, -1)
    X_df = pd.DataFrame(S_np, columns=state_cols_local)
    return behavior_model.predict_proba(X_df)

# Quick checking
try:
    with open(OUT_DIR / "rmab_sequences.pkl", "rb") as f:
        seqs_tmp = __import__("pickle").load(f)['sequences']
    sample_S = next(iter(seqs_tmp.values()))['S'][:5]
    print("b_probs shape (sample):", b_probs_from_state_array(sample_S).shape)
except Exception as e:
    print("Could not run quick sample check:", e)

# Cell X : FQE with STRONG REGULARIZATION (GBR)
import numpy as np, pickle, joblib, time
from pathlib import Path
from sklearn.ensemble import GradientBoostingRegressor
from scipy.special import softmax
from tqdm import tqdm

OUT_DIR = Path("/content/crmab_outputs")
assert (OUT_DIR / "rmab_sequences.pkl").exists(), "Run Cell C first to build rmab_sequences.pkl"

with open(OUT_DIR / "rmab_sequences.pkl", "rb") as f:
    data = pickle.load(f)
sequences = data["sequences"]
state_cols = data["state_cols"]

# Flatten transitions
S_list=[]; A_list=[]; R_list=[]; Snext_list=[]; done_list=[]
for seq in sequences.values():
    S, A, R = seq["S"], seq["A"], seq["R"]
    T = len(A)
    for t in range(T):
        S_list.append(S[t])
        A_list.append(A[t])
        R_list.append(R[t])
        if t == T-1:
            Snext_list.append(np.zeros_like(S[t]))
            done_list.append(True)
        else:
            Snext_list.append(S[t+1])
            done_list.append(False)

S_all = np.array(S_list); A_all = np.array(A_list); R_all = np.array(R_list)
Snext_all = np.array(Snext_list); done_all = np.array(done_list, dtype=bool)

actions = sorted(np.unique(A_all))
K = len(actions)

# STRONG regularized regressor
def make_reg_strong():
    return GradientBoostingRegressor(
        n_estimators=80,        # fewer trees
        learning_rate=0.05,     # lower learning rate
        max_depth=3,            # shallow trees
        min_samples_leaf=100,   # larger leaves reduce variance
        subsample=0.6,          # stochastic boosting
        random_state=42
    )

gamma = 0.95
temp  = 0.05
print(f"FQE (strong-reg): gamma={gamma}, temp={temp}, #samples={len(S_all)}, K={K}")

# Init Q models per action with immediate reward fit
Q = {a: make_reg_strong() for a in actions}
for a in actions:
    mask = (A_all == a)
    if mask.sum() > 0:
        Q[a].fit(S_all[mask], R_all[mask])
    else:
        Q[a].fit(S_all[:200], np.zeros(200))

def q_hat_batch(S, Qdict=Q, acts=actions):
    return np.vstack([Qdict[a].predict(S) for a in acts]).T  # (N,K)

# Policy evaluation iterations (FQE)
n_iters = 10
for it in range(n_iters):
    t0 = time.time()
    Q_next = q_hat_batch(Snext_all)                          # (N,K)
    pi_next = softmax(Q_next / temp, axis=1)                 # (N,K)
    V_next = (pi_next * Q_next).sum(axis=1)                  # (N,)
    targets = R_all + gamma * V_next * (~done_all)           # (N,)
    for a in actions:
        mask = (A_all == a)
        if mask.sum() > 0:
            Q[a].fit(S_all[mask], targets[mask])
    print(f"FQE iter {it+1}/{n_iters} done in {time.time()-t0:.1f}s")

# Saving FQE models (strong)
OUT_PATH = OUT_DIR / "q_models_fqe_strong.pkl"
joblib.dump(Q, OUT_PATH)
print("Saved FQE Q-models (strong) ->", OUT_PATH)

# Cell Y: DR with FQE Q (discounted, clipped)
import numpy as np, pickle, joblib, json
from pathlib import Path
from scipy.special import softmax
from tqdm import tqdm

OUT_DIR = Path("/content/crmab_outputs")
with open(OUT_DIR / "rmab_sequences.pkl", "rb") as f:
    rmab_data = pickle.load(f)
sequences = rmab_data.get('sequences', rmab_data)
state_cols = rmab_data.get('state_cols', None)
keys = list(sequences.keys())
print("Loaded sequences:", len(keys))

Q_models = joblib.load(OUT_DIR / "q_models_fqe_strong.pkl")


# behavior model (same as before)
if (OUT_DIR / "behavior_model_lgbm_statecols.pkl").exists():
    behavior_model = joblib.load(OUT_DIR / "behavior_model_lgbm_statecols.pkl")
else:
    behavior_model = joblib.load(OUT_DIR / "behavior_model_lgbm.pkl")

actions = sorted(list(Q_models.keys()))
gamma = 0.95
temp  = 0.05
clip_ratio = 20.0

def q_hat_batch(S):
    return np.vstack([Q_models[a].predict(S) for a in actions]).T

def compute_traj_dr_correct(S, A, R):
    import pandas as pd
    T = len(A)
    Xdf = pd.DataFrame(S, columns=state_cols) if state_cols is not None else pd.DataFrame(S)
    b = behavior_model.predict_proba(Xdf)                 # (T,K)
    q = q_hat_batch(S)                                    # (T,K)
    pi = softmax(q / temp, axis=1)                        # (T,K)
    V = (pi * q).sum(axis=1)                              # (T,)
    V_next = np.concatenate([V[1:], np.array([0.0])])
    a_idx = np.array(A, dtype=int)
    b_sel = np.maximum(b[np.arange(T), a_idx], 1e-12)
    pi_sel = pi[np.arange(T), a_idx]
    q_sel = q[np.arange(T), a_idx]
    residual = np.array(R) + gamma * V_next - q_sel
    w = np.clip(pi_sel / b_sel, 0.0, clip_ratio)
    discounts = (gamma ** np.arange(T))
    return float((discounts * (w * residual + V)).sum())

# Precompute DR per trajectory
vals = []
for k in tqdm(keys, desc="DR (FQE) per trajectory"):
    seq = sequences[k]
    vals.append(compute_traj_dr_correct(seq['S'], seq['A'], seq['R']))
vals = np.array(vals, float)
mean_return = vals.mean()
ci_lower = np.percentile(vals[np.random.randint(0, len(vals), (300, len(vals)))].mean(axis=1), 2.5)
ci_upper = np.percentile(vals[np.random.randint(0, len(vals), (300, len(vals)))].mean(axis=1), 97.5)

print(f"DR (with FQE Q) mean: {mean_return:.4f}  CI: [{ci_lower:.4f}, {ci_upper:.4f}]")

# Saving
summary = {
    "dr_mean_return_corrected_fqe": float(mean_return),
    "dr_ci_lower_corrected_fqe": float(ci_lower),
    "dr_ci_upper_corrected_fqe": float(ci_upper),
    "gamma": float(gamma),
    "clip_ratio": float(clip_ratio),
    "temp": float(temp),
    "n_trajectories": int(len(vals)),
}
(OUT_DIR / "fqi_ope_summary_dr_corrected_fqe.json").write_text(json.dumps(summary, indent=2))
print("Saved ->", OUT_DIR / "fqi_ope_summary_dr_corrected_fqe.json")

# CROSS-FIT OPE PIPELINE (BEST RECOMMENDATION)
# - K-fold cross-fit by trajectory (hadm_id, aug_id)
# - Train behavior model (LGBM) per-fold + calibrate (sigmoid)
# - Train FQE Q-models per-fold (GradientBoostingRegressor)
# - Evaluate on held-out fold: non-cumulative clipped DR, capped cumulative DR, WIS
# - Aggregate, bootstrap CI, ESS, diagnostics
# Paste into Colab in the same environment where /content/crmab_outputs exists.

import warnings, time, json, pickle, joblib
from pathlib import Path
import numpy as np, pandas as pd
from tqdm import tqdm
from sklearn.model_selection import GroupKFold
from sklearn.calibration import CalibratedClassifierCV
from scipy.special import softmax
from sklearn.ensemble import GradientBoostingRegressor
import lightgbm as lgb
import matplotlib.pyplot as plt

warnings.filterwarnings("ignore")
OUT_DIR = Path("/content/crmab_outputs")
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ----------------- User-tunable parameters -----------------
K_FOLDS = 5
TEMP = 0.2           # conservative policy softness (recommended)
CLIP = 5.0           # per-step clipping (recommended)
RHO_CAP = 1000.0     # cap for cumulative product (if used)
GAMMA = 0.95
FQE_ITERS = 10
BOOTSTRAP_B = 1000
SEED = 42
np.random.seed(SEED)

# ----------------- helper: robust calibrator constructor --------------
def make_calibrated(base_clf, method='sigmoid', cv=3):
    """Robust construction for CalibratedClassifierCV across sklearn versions."""
    try:
        return CalibratedClassifierCV(estimator=base_clf, method=method, cv=cv)
    except TypeError:
        try:
            return CalibratedClassifierCV(base_estimator=base_clf, method=method, cv=cv)
        except TypeError:
            return CalibratedClassifierCV(base_clf, method=method, cv=cv)

# ----------------- load sequences and dataframe ----------------------
RMAB_PKL = OUT_DIR / "rmab_sequences.pkl"
CLEAN_CSV = OUT_DIR / "final_traj_clean_training_heart_v2_sorted_imputed.csv"
assert RMAB_PKL.exists(), "Run earlier Cell C to produce rmab_sequences.pkl"
assert CLEAN_CSV.exists(), "Run Cell A to create the cleaned CSV"

with open(RMAB_PKL, "rb") as f:
    rmab = pickle.load(f)
sequences = rmab.get("sequences", rmab)
state_cols = rmab.get("state_cols")
traj_keys = list(sequences.keys())
n_traj = len(traj_keys)
print("Trajectories:", n_traj, "State dim:", len(state_cols))

df = pd.read_csv(CLEAN_CSV, low_memory=False)
# create a column 'traj_key' for fast row selection
df['traj_key'] = list(zip(df['hadm_id'].astype(int), df['aug_id'].astype(int)))

# ----------------- helper functions (FQE + estimators) ----------------
def make_reg_strong():
    return GradientBoostingRegressor(
        n_estimators=80, learning_rate=0.05, max_depth=3,
        min_samples_leaf=100, subsample=0.6, random_state=SEED
    )

def fit_fqe_on_trajectories(train_keys, sequences_local, n_iters=FQE_ITERS, temp_inner=TEMP):
    """Train FQE Q-models on transitions from train_keys (list of traj keys)."""
    # flatten transitions
    S_list=[]; A_list=[]; R_list=[]; Snext_list=[]; done_list=[]
    for k in train_keys:
        seq = sequences_local[k]
        S, A, R = np.array(seq['S'],dtype=float), np.array(seq['A'],dtype=int), np.array(seq['R'],dtype=float)
        T = len(A)
        for t in range(T):
            S_list.append(S[t])
            A_list.append(A[t])
            R_list.append(R[t])
            if t == T-1:
                Snext_list.append(np.zeros_like(S[t])); done_list.append(True)
            else:
                Snext_list.append(S[t+1]); done_list.append(False)
    if len(S_list) == 0:
        raise RuntimeError("No transitions in training set for FQE!")
    S_all = np.array(S_list); A_all = np.array(A_list); R_all = np.array(R_list)
    Snext_all = np.array(Snext_list); done_all = np.array(done_list, dtype=bool)
    actions = sorted(np.unique(A_all))
    # init Q models
    Q = {a: make_reg_strong() for a in actions}
    # initial fit to immediate rewards
    for a in actions:
        mask = (A_all == a)
        if mask.sum() > 0:
            Q[a].fit(S_all[mask], R_all[mask])
        else:
            Q[a].fit(S_all[:200], np.zeros(min(200, len(S_all))))
    # FQE iters
    def q_hat_batch_local(S_np):
        return np.vstack([Q[a].predict(S_np) for a in actions]).T
    for _ in range(n_iters):
        Q_next = q_hat_batch_local(Snext_all)
        pi_next = softmax(Q_next / temp_inner, axis=1)
        V_next = (pi_next * Q_next).sum(axis=1)
        targets = R_all + GAMMA * V_next * (~done_all)
        for a in actions:
            mask = (A_all == a)
            if mask.sum() > 0:
                Q[a].fit(S_all[mask], targets[mask])
    return Q, actions

def q_hat_batch_from_models(S_np, Q_models_local, actions_sorted_local):
    return np.vstack([Q_models_local[a].predict(S_np) for a in actions_sorted_local]).T

def per_traj_noncum_dr(S, A, R, behavior_model, Q_models_local, actions_sorted_local, temp=TEMP, clip_ratio=CLIP):
    """Original-style safer per-step clipped DR (no cumulative product)."""
    Xdf = pd.DataFrame(S, columns=state_cols)
    b = behavior_model.predict_proba(Xdf)
    q = q_hat_batch_from_models(S, Q_models_local, actions_sorted_local)
    pi = softmax(q / temp, axis=1)
    V = (pi * q).sum(axis=1); V_next = np.concatenate([V[1:], np.array([0.0])])
    a_idx = np.array(A, dtype=int)
    b_sel = np.maximum(b[np.arange(len(A)), a_idx], 1e-12)
    pi_sel = pi[np.arange(len(A)), a_idx]
    q_sel = q[np.arange(len(A)), a_idx]
    residual = np.array(R) + GAMMA * V_next - q_sel
    w = np.clip(pi_sel / b_sel, 0.0, clip_ratio)  # elementwise clipping
    discounts = (GAMMA ** np.arange(len(A)))
    return float((discounts * (w * residual + V)).sum())

def per_traj_capped_cum_dr(S, A, R, behavior_model, Q_models_local, actions_sorted_local, temp=TEMP, clip_ratio=CLIP, rho_cap=RHO_CAP):
    """Capped cumulative product per-decision DR (rho_t capped)."""
    Xdf = pd.DataFrame(S, columns=state_cols)
    b = behavior_model.predict_proba(Xdf)
    q = q_hat_batch_from_models(S, Q_models_local, actions_sorted_local)
    pi = softmax(q / temp, axis=1)
    a_idx = np.array(A, dtype=int)
    b_sel = np.maximum(b[np.arange(len(A)), a_idx], 1e-12)
    pi_sel = np.maximum(pi[np.arange(len(A)), a_idx], 1e-12)
    q_sel = q[np.arange(len(A)), a_idx]
    ratio_step = np.clip(pi_sel / b_sel, 0.0, clip_ratio)
    rho = np.ones(len(A), dtype=float)
    for t in range(len(A)):
        if t == 0:
            rho[t] = min(ratio_step[t], rho_cap)
        else:
            rho[t] = min(rho[t-1] * ratio_step[t], rho_cap)
    V = (pi * q).sum(axis=1)
    V_next = np.concatenate([V[1:], np.array([0.0])])
    discounts = (GAMMA ** np.arange(len(A)))
    dr_steps = rho * (R + GAMMA * V_next - q_sel) + rho * V
    return float((discounts * dr_steps).sum()), float(rho[-1])

def traj_weight_and_disc_return(S, A, R, behavior_model, Q_models_local, actions_sorted_local, temp=TEMP, clip_ratio=CLIP):
    """Compute per-trajectory product weight w_prod (clipped per-step) and discounted return."""
    Xdf = pd.DataFrame(S, columns=state_cols)
    b = behavior_model.predict_proba(Xdf)
    q = q_hat_batch_from_models(S, Q_models_local, actions_sorted_local)
    pi = softmax(q / temp, axis=1)
    a_idx = np.array(A, dtype=int)
    b_sel = np.maximum(b[np.arange(len(A)), a_idx], 1e-12)
    pi_sel = np.maximum(pi[np.arange(len(A)), a_idx], 1e-12)
    ratio = np.clip(pi_sel / b_sel, 0.0, clip_ratio)
    w_prod = float(np.prod(ratio))
    disc_return = float((GAMMA ** np.arange(len(R)) * np.array(R)).sum())
    return w_prod, disc_return

# ----------------- cross-fold loop ------------------------------------
print(f"\nStarting {K_FOLDS}-fold cross-fit (this may take a while)...")
# Build trajectory-level arrays for grouping: per trajectory, we need hadm_id to do GroupKFold
traj_hadm = [int(k[0]) for k in traj_keys]
traj_idx = np.arange(n_traj)
gkf = GroupKFold(n_splits=K_FOLDS)
fold_assign = np.zeros(n_traj, dtype=int)
for fold_num, (_, te_idx) in enumerate(gkf.split(traj_idx, groups=traj_hadm)):
    fold_assign[te_idx] = fold_num

# store per-traj results
records = []

for fold in range(K_FOLDS):
    t0_fold = time.time()
    print(f"\n--- Fold {fold+1}/{K_FOLDS} ---")
    # split keys
    te_idx = np.where(fold_assign == fold)[0]
    tr_idx = np.where(fold_assign != fold)[0]
    train_keys = [traj_keys[i] for i in tr_idx]
    test_keys = [traj_keys[i] for i in te_idx]
    print(" train trajs:", len(train_keys), " test trajs:", len(test_keys))

    # 1) Train behavior model on train_rows (all timesteps from train_keys)
    train_key_set = set(train_keys)
    mask_train_rows = df['traj_key'].isin(train_key_set)
    X_train = df.loc[mask_train_rows, state_cols].apply(pd.to_numeric, errors='coerce').fillna(0.0)
    y_train = df.loc[mask_train_rows, 'action_mapped'].astype(int).values
    if len(X_train) == 0:
        raise RuntimeError("No training rows for behavior model in this fold!")
    base_clf = lgb.LGBMClassifier(objective="multiclass", num_class=len(df['action_mapped'].unique()), learning_rate=0.08, n_estimators=200, num_leaves=31, random_state=SEED)
    base_clf.fit(X_train, y_train)
    # calibrate on train set (cv=3)
    cal = make_calibrated(base_clf, method='sigmoid', cv=3)
    cal.fit(X_train, y_train)
    # save per-fold calibrator (optional)
    joblib.dump(cal, OUT_DIR / f"behavior_cal_fold_{fold}.pkl")

    # 2) Train FQE on train_keys
    Q_models_fold, actions_fold = fit_fqe_on_trajectories(train_keys, sequences, n_iters=FQE_ITERS, temp_inner=TEMP)
    actions_sorted_fold = sorted(actions_fold)

    # 3) Evaluate on test_keys (per-trajectory)
    for key in tqdm(test_keys, desc=f" eval fold {fold+1}"):
        seq = sequences[key]
        S = np.array(seq['S'], dtype=float); A = np.array(seq['A'], dtype=int); R = np.array(seq['R'], dtype=float)
        # safer non-cumulative DR
        try:
            dr_noncum = per_traj_noncum_dr(S, A, R, cal, Q_models_fold, actions_sorted_fold, temp=TEMP, clip_ratio=CLIP)
        except Exception as e:
            dr_noncum = float('nan')
        # capped cumulative DR + final rho
        try:
            dr_cum, final_rho = per_traj_capped_cum_dr(S, A, R, cal, Q_models_fold, actions_sorted_fold, temp=TEMP, clip_ratio=CLIP, rho_cap=RHO_CAP)
        except Exception as e:
            dr_cum, final_rho = float('nan'), 0.0
        # product weight & discounted return
        try:
            wprod, discret = traj_weight_and_disc_return(S, A, R, cal, Q_models_fold, actions_sorted_fold, temp=TEMP, clip_ratio=CLIP)
        except Exception as e:
            wprod, discret = 0.0, float((GAMMA ** np.arange(len(R)) * R).sum())
        records.append({
            "key": key,
            "fold": int(fold),
            "len": int(len(A)),
            "dr_noncum": float(dr_noncum),
            "dr_capped_cum": float(dr_cum),
            "final_rho": float(final_rho),
            "w_prod": float(wprod),
            "disc_return": float(discret)
        })

    print("Fold time:", time.time() - t0_fold, "s")

# convert to DataFrame
rec_df = pd.DataFrame(records)
print("\nCollected per-trajectory results:", rec_df.shape)

# ----------------- Aggregate & compute estimators ----------------------
# 1) mean of per-traj non-cumulative DR (simple average)
dr_noncum_mean = rec_df['dr_noncum'].mean()
# 2) mean of capped cumulative DR
dr_cum_mean = rec_df['dr_capped_cum'].mean()
# 3) WIS (self-normalized IS) using w_prod and disc_return
w_arr = rec_df['w_prod'].values
r_arr = rec_df['disc_return'].values
wis = float((w_arr * r_arr).sum() / (w_arr.sum() + 1e-12))
# 4) ESS (trajectory-level)
def compute_ess(weights):
    w = np.asarray(weights, dtype=float)
    denom = (w**2).sum()
    return float((w.sum()**2) / denom) if denom > 0 else 0.0
ess_traj = compute_ess(w_arr)

print("\nAggregate results (no bootstrap):")
print(" non-cumulative DR mean:", dr_noncum_mean)
print(" capped cumulative DR mean:", dr_cum_mean)
print(" WIS (temp,clip):", wis)
print(" ESS (traj-level):", ess_traj)

# ----------------- Bootstrap CI (trajectory-level resampling) ----------
print(f"\nRunning bootstrap B={BOOTSTRAP_B} resamples for CI (this may take time)...")
rng = np.random.default_rng(SEED)
n = len(rec_df)
dr_noncum_bs = []
dr_cum_bs = []
wis_bs = []
for _ in tqdm(range(BOOTSTRAP_B)):
    idx = rng.integers(0, n, n)
    sample = rec_df.iloc[idx]
    dr_noncum_bs.append(sample['dr_noncum'].mean())
    dr_cum_bs.append(sample['dr_capped_cum'].mean())
    w_s = sample['w_prod'].values; r_s = sample['disc_return'].values
    wis_bs.append(float((w_s * r_s).sum() / (w_s.sum() + 1e-12)))
dr_noncum_ci = np.percentile(dr_noncum_bs, [2.5,97.5])
dr_cum_ci = np.percentile(dr_cum_bs, [2.5,97.5])
wis_ci = np.percentile(wis_bs, [2.5,97.5])

print("\nBootstrap CIs:")
print(" non-cum DR mean CI:", dr_noncum_ci)
print(" capped-cum DR mean CI:", dr_cum_ci)
print(" WIS CI:", wis_ci)

# ----------------- Diagnostics & save ---------------------------------
# Save per-traj table and summary JSON
rec_df.to_csv(OUT_DIR / "ope_crossfit_per_traj.csv", index=False)

summary = {
    "temp": float(TEMP), "clip": float(CLIP), "rho_cap": float(RHO_CAP),
    "gamma": float(GAMMA), "n_traj": int(n_traj),
    "dr_noncum_mean": float(dr_noncum_mean),
    "dr_noncum_ci_low": float(dr_noncum_ci[0]),
    "dr_noncum_ci_high": float(dr_noncum_ci[1]),
    "dr_cum_mean": float(dr_cum_mean),
    "dr_cum_ci_low": float(dr_cum_ci[0]),
    "dr_cum_ci_high": float(dr_cum_ci[1]),
    "wis": float(wis),
    "wis_ci_low": float(wis_ci[0]),
    "wis_ci_high": float(wis_ci[1]),
    "ess_traj": float(ess_traj)
}
(OUT_DIR / "ope_crossfit_summary.json").write_text(json.dumps(summary, indent=2))
pd.DataFrame([summary]).to_csv(OUT_DIR / "ope_crossfit_results.csv", index=False)

# Plot weight histogram (log scale) and save
w_vals = rec_df['w_prod'].values
plt.figure(figsize=(6,3)); plt.hist(np.log1p(w_vals), bins=200); plt.title("log1p trajectory product weights"); plt.tight_layout()
plt.savefig(OUT_DIR / "weight_hist_log1p.png")
print("Saved weight histogram ->", OUT_DIR / "weight_hist_log1p.png")

# Top offenders by final_rho (if you used capped cumulative DR) and by w_prod
top_rho = rec_df.sort_values('final_rho', ascending=False).head(10)
top_w = rec_df.sort_values('w_prod', ascending=False).head(10)
print("\nTop trajectories by final_rho (capped cum):")
display(top_rho)
print("\nTop trajectories by w_prod (product weight):")
display(top_w)

print("\nSaved per-trajectory and summary outputs to:", OUT_DIR)
print("Primary recommendation: report WIS (self-normalized IS) and non-cumulative clipped DR as robustness check.")

import pickle, joblib, pandas as pd
OUT_DIR = Path("/content/crmab_outputs")
rec = pd.read_csv(OUT_DIR/"ope_crossfit_per_traj.csv")
top_w = rec.sort_values("w_prod", ascending=False).head(10)
print(top_w[['key','fold','len','w_prod','final_rho','disc_return','dr_noncum','dr_capped_cum']])
# To print full S/A/R for the top one:
sequences = pickle.load(open(OUT_DIR/"rmab_sequences.pkl","rb"))['sequences']
k0 = top_w.iloc[0]['key']
print("Top key:", k0)
traj = sequences[eval(k0)]   # keys are tuples serialized like "(hadm, aug)" in CSV
print(traj['S'])
print(traj['A'])
print(traj['R'])

import numpy as np, matplotlib.pyplot as plt
w = rec['w_prod'].values
plt.figure(figsize=(6,3))
plt.hist(np.log1p(w), bins=200)
plt.xlim(0,8)   # focus to see tail within 8
plt.title("log1p trajectory weights (zoom)")
plt.show()

import pandas as pd, numpy as np
from pathlib import Path
OUT_DIR = Path("/content/crmab_outputs")
rec = pd.read_csv(OUT_DIR/"ope_crossfit_per_traj.csv")
V_target_WIS = (rec.w_prod * rec.disc_return).sum() / (rec.w_prod.sum() + 1e-12)
V_behavior = rec.disc_return.mean()
uplift = V_target_WIS - V_behavior
print(f"Behavior value (on-policy est): {V_behavior:.4f}")
print(f"Target value (WIS): {V_target_WIS:.4f}")
print(f"Estimated uplift: {uplift:.4f}")

# === TURBO v2: Fast, Safe, and Transparent OPE with Live ETA ===
import warnings, json, pickle, joblib, numpy as np, pandas as pd, time, math, os
from pathlib import Path
from tqdm import tqdm
from sklearn.model_selection import GroupKFold
from sklearn.calibration import CalibratedClassifierCV
from scipy.special import softmax
import lightgbm as lgb

warnings.filterwarnings("ignore")
os.environ["OMP_NUM_THREADS"] = "4"  # keep CPU responsive; tweak if you have more cores
OUT_DIR = Path("/content/crmab_outputs")
assert (OUT_DIR/"rmab_sequences.pkl").exists(), "Missing rmab_sequences.pkl"
assert (OUT_DIR/"final_traj_clean_training_heart_v2_sorted_imputed.csv").exists(), "Missing cleaned CSV"

# ---------------- Config (balanced speed + safety) ----------------
K_FOLDS          = 5
TUNE_SAMPLE_N    = 6000   # number of trajectories for quick grid selection (safe subset)
TEMP_GRID        = [0.20, 0.25]      # conservative temps
ALPHAS           = [0.0, 0.2, 0.4]   # policy mix with behavior
TAUS             = [0.0, 0.01]       # support floor
CLIP             = 3.0
GAMMA            = 0.95
FQE_ITERS_TUNE   = 6      # fewer iters for quick tuning
FQE_ITERS_FULL   = 8      # still moderate, good regularization
SEED             = 42
np.random.seed(SEED)

# ---------------- Load data ----------------
with open(OUT_DIR/"rmab_sequences.pkl", "rb") as f:
    rmab = pickle.load(f)
sequences = rmab["sequences"]; state_cols = rmab["state_cols"]
traj_keys = list(sequences.keys()); n_traj = len(traj_keys)

df = pd.read_csv(OUT_DIR/"final_traj_clean_training_heart_v2_sorted_imputed.csv", low_memory=False)
df["traj_key"] = list(zip(df["hadm_id"].astype(int), df["aug_id"].astype(int)))
num_classes = df["action_mapped"].nunique()

# ---------------- Helpers ----------------
def fmt_eta(s):
    s = int(max(0, s)); m, s = divmod(s, 60); h, m = divmod(m, 60)
    return (f"{h}h {m}m {s}s" if h else f"{m}m {s}s") if m else f"{s}s"

def make_Q_regressor():
    # LightGBM Regressor -> much faster than sklearn GBR
    return lgb.LGBMRegressor(
        objective="regression",
        learning_rate=0.05,
        n_estimators=180,
        num_leaves=31,
        subsample=0.6,
        colsample_bytree=0.8,
        reg_alpha=0.0,
        reg_lambda=2.0,
        min_data_in_leaf=200,
        n_jobs=-1,
        random_state=SEED
    )

def flatten_transitions(keys):
    S_list=[]; A_list=[]; R_list=[]; Snext_list=[]; done_list=[]
    for k in keys:
        seq = sequences[k]
        S, A, R = np.asarray(seq["S"]), np.asarray(seq["A"], int), np.asarray(seq["R"], float)
        T = len(A)
        for t in range(T):
            S_list.append(S[t]); A_list.append(A[t]); R_list.append(R[t])
            if t == T-1:
                Snext_list.append(np.zeros_like(S[t])); done_list.append(True)
            else:
                Snext_list.append(S[t+1]); done_list.append(False)
    return (np.array(S_list), np.array(A_list), np.array(R_list),
            np.array(Snext_list), np.array(done_list, bool))

def fit_fqe_LGB(train_keys, iters, temp_inner):
    S_all, A_all, R_all, Snext_all, done_all = flatten_transitions(train_keys)
    actions = sorted(np.unique(A_all))
    Q = {a: make_Q_regressor() for a in actions}
    # warm start with immediate rewards
    for a in actions:
        m = (A_all==a)
        if m.any():
            Q[a].fit(S_all[m], R_all[m])
        else:
            Q[a].fit(S_all[:200], np.zeros(min(200, len(S_all))))
    # iterative fitted Q
    def q_hat(S):
        return np.vstack([Q[a].predict(S) for a in actions]).T
    for _ in range(iters):
        Qn = q_hat(Snext_all)
        pi_n = softmax(Qn / temp_inner, axis=1)
        Vn = (pi_n * Qn).sum(axis=1)
        targets = R_all + GAMMA * Vn * (~done_all)
        for a in actions:
            m = (A_all==a)
            if m.any():
                Q[a].fit(S_all[m], targets[m])
    return Q, actions

def behavior_model_fold(train_keys, method="sigmoid"):
    # Train base, then calibrate
    mask = df["traj_key"].isin(set(train_keys))
    X = df.loc[mask, state_cols].apply(pd.to_numeric, errors="coerce").fillna(0.0)
    y = df.loc[mask, "action_mapped"].astype(int).values
    base = lgb.LGBMClassifier(
        objective="multiclass",
        num_class=num_classes,
        learning_rate=0.08,
        n_estimators=250,
        num_leaves=31,
        subsample=0.9,
        colsample_bytree=0.8,
        random_state=SEED
    )
    base.fit(X, y)
    try:
        cal = CalibratedClassifierCV(base, method=method, cv=3)
    except TypeError:
        # sklearn < 1.4 compatibility
        cal = CalibratedClassifierCV(base_estimator=base, method=method, cv=3)
    cal.fit(X, y)
    return cal

def precompute_b_q(test_keys, cal, Q_models, actions_sorted, stage_name="precompute"):
    # progress for precompute
    t0 = time.time()
    pre = {}
    bar = tqdm(test_keys, desc=f"{stage_name}: b,Q per traj", unit="traj")
    for key in bar:
        seq = sequences[key]
        S = pd.DataFrame(seq["S"], columns=state_cols)
        A = np.asarray(seq["A"], int); R = np.asarray(seq["R"], float)
        b = cal.predict_proba(S)  # (T,K)
        q = np.vstack([Q_models[a].predict(S.values) for a in actions_sorted]).T
        disc_ret = float((GAMMA**np.arange(len(R)) * R).sum())
        pre[key] = {"b": b, "q": q, "A": A, "ret": disc_ret}
    print(f"‚úÖ {stage_name} done in {fmt_eta(time.time()-t0)}")
    return pre

def wis_from_pre(pre, temp, alpha, tau, clip=CLIP):
    ratios=[]; returns=[]
    for key, obj in pre.items():
        b, q, A, ret = obj["b"], obj["q"], obj["A"], obj["ret"]
        pi = softmax(q / temp, axis=1)
        if tau > 0.0:
            mask = (b < tau)
            pi = np.where(mask, 0.0, pi)
            s = pi.sum(1, keepdims=True)
            pi = np.where(s>0, pi/s, b)  # fallback to behavior
        pi_mix = (1.0 - alpha)*b + alpha*pi
        T = len(A); idx = np.arange(T)
        b_sel  = np.maximum(b[idx, A], 1e-12)
        pi_sel = np.maximum(pi_mix[idx, A], 1e-12)
        ratio = np.clip(pi_sel/b_sel, 0.0, clip)
        ratios.append(float(np.prod(ratio)))
        returns.append(ret)
    w = np.array(ratios); r = np.array(returns)
    wis = float((w*r).sum() / (w.sum() + 1e-12))
    ess = float((w.sum()**2) / (np.sum(w**2) + 1e-12))
    return wis, ess

# ---------------- Split folds ----------------
traj_hadm = [int(k[0]) for k in traj_keys]
idx = np.arange(n_traj)
gkf = GroupKFold(n_splits=K_FOLDS)

# ---------------- Phase 1: Quick tuning on subset ----------------
print(f"üö¶ Phase 1 ‚Äî quick tuning on subset of {TUNE_SAMPLE_N} / {n_traj} trajectories")
rows_tune = []
phase1_t0 = time.time()

for fold_no, (tr_i, te_i) in enumerate(gkf.split(idx, groups=traj_hadm), 1):
    tr_keys = [traj_keys[i] for i in tr_i]
    te_keys_full = [traj_keys[i] for i in te_i]
    # sample a subset of test trajectories for tuning
    if len(te_keys_full) > TUNE_SAMPLE_N // K_FOLDS:
        rng = np.random.default_rng(SEED+fold_no)
        sample_idx = rng.choice(len(te_keys_full), size=TUNE_SAMPLE_N//K_FOLDS, replace=False)
        te_keys = [te_keys_full[i] for i in sample_idx]
    else:
        te_keys = te_keys_full

    print(f"\nüß≠ Fold {fold_no}/{K_FOLDS}  train={len(tr_keys)}  tune-test={len(te_keys)}")

    # Behavior calibration (sigmoid; isotonic is slower)
    cal = behavior_model_fold(tr_keys, method="sigmoid")

    # FQE (fewer iters for tuning)
    Q_models, actions = fit_fqe_LGB(tr_keys, iters=FQE_ITERS_TUNE, temp_inner=0.25)
    actions_sorted = sorted(actions)

    # Precompute b and Q for tuning subset
    pre = precompute_b_q(te_keys, cal, Q_models, actions_sorted, stage_name="Precompute[Tune]")

    # Grid on subset (fast)
    total_cfgs = len(TEMP_GRID)*len(ALPHAS)*len(TAUS)
    pbar = tqdm(total=total_cfgs, desc=f"Grid eval (fold {fold_no})", unit="cfg")
    start = time.time()
    done = 0
    for temp in TEMP_GRID:
        for alpha in ALPHAS:
            for tau in TAUS:
                wis, ess = wis_from_pre(pre, temp, alpha, tau, clip=CLIP)
                rows_tune.append({"fold":fold_no, "temp":temp, "alpha":alpha, "tau":tau, "wis":wis, "ess":ess})
                done += 1; pbar.update(1)
                elapsed = time.time()-start
                eta = elapsed * (total_cfgs-done) / max(done,1)
                if done == 1 or done == total_cfgs or done % max(1,total_cfgs//6)==0:
                    print(f"[{100*done/total_cfgs:4.1f}%] ETA {fmt_eta(eta)}")
    pbar.close()

grid_tune = pd.DataFrame(rows_tune)
grid_tune.to_csv(OUT_DIR/"policy_mix_grid_wis_TUNE.csv", index=False)

agg_tune = grid_tune.groupby(["temp","alpha","tau"]).agg(wis=("wis","mean"), ess=("ess","mean")).reset_index()
best_tune = agg_tune.sort_values(["wis","ess"], ascending=[False, False]).iloc[0]
print("\nüèÅ Phase 1 best (subset):", dict(best_tune))
print(f"‚è±Ô∏è Phase 1 time: {fmt_eta(time.time()-phase1_t0)}")

# ---------------- Phase 2: Full evaluation with selected config ----------------
SEL_TEMP = float(best_tune["temp"])
SEL_ALPHA = float(best_tune["alpha"])
SEL_TAU = float(best_tune["tau"])
print(f"\nüöÄ Phase 2 ‚Äî full eval with TEMP={SEL_TEMP}, ALPHA={SEL_ALPHA}, TAU={SEL_TAU}")

rows_full = []
phase2_t0 = time.time()
for fold_no, (tr_i, te_i) in enumerate(gkf.split(idx, groups=traj_hadm), 1):
    tr_keys = [traj_keys[i] for i in tr_i]
    te_keys = [traj_keys[i] for i in te_i]
    print(f"\nüß≠ Fold {fold_no}/{K_FOLDS}  train={len(tr_keys)}  test={len(te_keys)}")

    # Behavior calibration (sigmoid; faster and usually enough after mixing/support)
    cal = behavior_model_fold(tr_keys, method="sigmoid")

    # Slightly stronger FQE for final numbers
    Q_models, actions = fit_fqe_LGB(tr_keys, iters=FQE_ITERS_FULL, temp_inner=0.25)
    actions_sorted = sorted(actions)

    # Precompute b, Q for ALL test trajectories (progress visible)
    pre = precompute_b_q(te_keys, cal, Q_models, actions_sorted, stage_name="Precompute[Full]")

    # Single config evaluation on full test set
    print("Evaluating selected config on fold test set ...")
    wis, ess = wis_from_pre(pre, SEL_TEMP, SEL_ALPHA, SEL_TAU, clip=CLIP)
    rows_full.append({"fold":fold_no, "wis":wis, "ess":ess})

grid_full = pd.DataFrame(rows_full)
mu_wis = grid_full["wis"].mean(); sd_wis = grid_full["wis"].std(ddof=1)
ci_low = mu_wis - 1.96*sd_wis/np.sqrt(K_FOLDS)
ci_high = mu_wis + 1.96*sd_wis/np.sqrt(K_FOLDS)
mu_ess = grid_full["ess"].mean()

# Behavior on-policy value for context
beh_disc_vals = []
for k in sequences:
    R = np.array(sequences[k]["R"], float)
    beh_disc_vals.append(float((GAMMA**np.arange(len(R)) * R).sum()))
V_behavior = float(np.mean(beh_disc_vals))
uplift = mu_wis - V_behavior

# Save outputs
grid_full.to_csv(OUT_DIR/"policy_mix_selected_FULL.csv", index=False)
sel_summary = {
    "temp": SEL_TEMP, "alpha": SEL_ALPHA, "tau": SEL_TAU,
    "clip": CLIP, "gamma": GAMMA,
    "wis_mean": float(mu_wis), "wis_ci_low": float(ci_low), "wis_ci_high": float(ci_high),
    "ess_mean": float(mu_ess),
    "behavior_value": float(V_behavior),
    "estimated_uplift": float(uplift),
    "phase1_grid_file": str(OUT_DIR/"policy_mix_grid_wis_TUNE.csv"),
    "phase2_results_file": str(OUT_DIR/"policy_mix_selected_FULL.csv")
}
(OUT_DIR/"policy_mix_selected_summary.json").write_text(json.dumps(sel_summary, indent=2))

print("\n================ RESULTS ================")
print(f"Best (subset-tuned): TEMP={SEL_TEMP}, ALPHA={SEL_ALPHA}, TAU={SEL_TAU}")
print(f"WIS (full, fold-mean): {mu_wis:.4f}  CI‚âà[{ci_low:.4f}, {ci_high:.4f}]")
print(f"ESS (mean across folds): {mu_ess:.1f}")
print(f"Behavior value (on-policy est): {V_behavior:.4f}")
print(f"Estimated uplift (WIS - behavior): {uplift:+.4f}")
print(f"Phase 1 time + Phase 2 time: {fmt_eta(time.time()-phase1_t0)} + {fmt_eta(time.time()-phase2_t0)}")
print("Files saved to:", OUT_DIR)

# === Phase-2: Re-evaluate selected policy using ISOTONIC calibration ===
import warnings, json, pickle, joblib, numpy as np, pandas as pd, time
from pathlib import Path
from tqdm import tqdm
from sklearn.model_selection import GroupKFold
from sklearn.calibration import CalibratedClassifierCV
from scipy.special import softmax
import lightgbm as lgb

warnings.filterwarnings("ignore")

OUT_DIR = Path("/content/crmab_outputs")
CFG = json.loads((OUT_DIR/"policy_mix_selected_summary.json").read_text())
SEL_TEMP, SEL_ALPHA, SEL_TAU = CFG["temp"], CFG["alpha"], CFG["tau"]
GAMMA, CLIP = CFG["gamma"], 3.0
K_FOLDS, SEED = 5, 42
np.random.seed(SEED)

# Load sequences + dataframe
with open(OUT_DIR/"rmab_sequences.pkl","rb") as f:
    rmab = pickle.load(f)

sequences, state_cols = rmab["sequences"], rmab["state_cols"]
traj_keys = list(sequences.keys())
df = pd.read_csv(OUT_DIR/"final_traj_clean_training_heart_v2_sorted_imputed.csv")
df["traj_key"] = list(zip(df.hadm_id.astype(int), df.aug_id.astype(int)))
num_classes = df["action_mapped"].nunique()

# ---------- Utility funcs ----------
def flatten_trajs(keys):
    S,A,R,S2,D = [],[],[],[],[]
    for k in keys:
        seq = sequences[k]
        s,a,r = seq["S"], seq["A"], seq["R"]
        T = len(a)
        for t in range(T):
            S.append(s[t]); A.append(a[t]); R.append(r[t])
            if t==T-1:
                S2.append(np.zeros_like(s[t])); D.append(True)
            else:
                S2.append(s[t+1]); D.append(False)
    return map(np.array,(S,A,R,S2,np.array(D,bool)))

def make_Q():
    return lgb.LGBMRegressor(
        objective="regression",
        n_estimators=160, learning_rate=0.05,
        num_leaves=31, subsample=0.6,
        colsample_bytree=0.8, min_data_in_leaf=200,
        random_state=SEED
    )

def fit_fqe(keys, temp_inner=0.25, iters=6):
    S,A,R,S2,D = flatten_trajs(keys)
    actions = sorted(np.unique(A))
    Q = {a: make_Q() for a in actions}

    # Init on reward
    for a in actions:
        m = (A==a)
        if m.any():
            Q[a].fit(S[m], R[m])
        else:
            Q[a].fit(S[:200], np.zeros(200))

    # FQE iterations
    def q_hat(x): return np.vstack([Q[a].predict(x) for a in actions]).T
    for _ in range(iters):
        Qn = q_hat(S2)
        pi_n = softmax(Qn/temp_inner, axis=1)
        Vn = (pi_n * Qn).sum(axis=1)
        Targ = R + GAMMA * Vn * (~D)
        for a in actions:
            m = (A==a)
            if m.any(): Q[a].fit(S[m], Targ[m])
    return Q, actions

def precompute(test_keys, cal, Q, actions):
    pre = {}
    for k in tqdm(test_keys, desc="Precompute [ISO]"):
        seq = sequences[k]
        S = pd.DataFrame(seq["S"], columns=state_cols)
        A = np.array(seq["A"], int)
        R = np.array(seq["R"], float)
        b = cal.predict_proba(S)
        q = np.vstack([Q[a].predict(S.values) for a in actions]).T
        ret = float((GAMMA**np.arange(len(R)) * R).sum())
        pre[k] = dict(b=b, q=q, A=A, ret=ret)
    return pre

def wis_from_pre(pre):
    ws, rs = [], []
    for p in pre.values():
        b,q,A,ret = p["b"],p["q"],p["A"],p["ret"]
        pi = softmax(q/SEL_TEMP, axis=1)

        # support truncation
        mask = b < SEL_TAU
        pi = np.where(mask, 0.0, pi)
        z = pi.sum(1, keepdims=True)
        pi = np.where(z>0, pi/z, b)

        pi_mix = (1-SEL_ALPHA)*b + SEL_ALPHA*pi

        idx = np.arange(len(A))
        b_sel  = np.maximum(b[idx, A], 1e-12)
        pi_sel = np.maximum(pi_mix[idx, A], 1e-12)
        w = np.clip(pi_sel/b_sel, 0, CLIP)

        ws.append(np.prod(w))
        rs.append(ret)

    w, r = np.array(ws), np.array(rs)
    wis = (w*r).sum() / (w.sum() + 1e-12)
    ess = (w.sum()**2) / (np.sum(w**2) + 1e-12)
    return wis, ess

# ---------- Cross-fold evaluation ----------
traj_hadm = [int(k[0]) for k in traj_keys]
gkf = GroupKFold(K_FOLDS)

rows=[]
for fold,(tr_i,te_i) in enumerate(gkf.split(traj_keys, groups=traj_hadm),1):
    tr_keys = [traj_keys[i] for i in tr_i]
    te_keys = [traj_keys[i] for i in te_i]

    print(f"\nüîÅ Fold {fold}/{K_FOLDS} | Train {len(tr_keys)}, Test {len(te_keys)}")

    # Train behavior + isotonic
    m = df["traj_key"].isin(set(tr_keys))
    Xtr = df.loc[m, state_cols].apply(pd.to_numeric).fillna(0.0)
    ytr = df.loc[m, "action_mapped"].astype(int)

    base = lgb.LGBMClassifier(
        objective="multiclass", num_class=num_classes,
        learning_rate=0.08, n_estimators=250,
        num_leaves=31, subsample=0.9,
        colsample_bytree=0.8, random_state=SEED
    )
    base.fit(Xtr, ytr)

    try:
        cal = CalibratedClassifierCV(base, method="isotonic", cv=3)
    except:
        cal = CalibratedClassifierCV(base_estimator=base, method="isotonic", cv=3)

    cal.fit(Xtr, ytr)

    # FQE
    Q, actions = fit_fqe(tr_keys)

    # Precompute and score
    pre = precompute(te_keys, cal, Q, actions)
    wis, ess = wis_from_pre(pre)

    rows.append(dict(fold=fold, wis=wis, ess=ess))
    print(f"‚úÖ Fold {fold} WIS={wis:.3f}, ESS={ess:.1f}")

# Aggregate
df_iso = pd.DataFrame(rows)
mean, sd = df_iso.wis.mean(), df_iso.wis.std(ddof=1)
ci = 1.96 * sd/np.sqrt(K_FOLDS)

print("\nüéØ **Isotonic Cross-fit Result**")
print(f"WIS = {mean:.4f}  (CI [{mean-ci:.4f}, {mean+ci:.4f}])")
print(f"ESS ‚âà {df_iso.ess.mean():.1f}")

(OUT_DIR/"policy_mix_selected_full_ISOTONIC.json").write_text(
    json.dumps({"wis":mean, "ci":[mean-ci, mean+ci], "ess":df_iso.ess.mean()}, indent=2)
)

# Cell D0 ‚Äî save isotonic-calibrated behavior model on FULL data
import pandas as pd, numpy as np, json, joblib
from pathlib import Path
import lightgbm as lgb
from sklearn.calibration import CalibratedClassifierCV

OUT_DIR = Path("/content/crmab_outputs")
assert (OUT_DIR/"final_traj_clean_training_heart_v2_sorted_imputed.csv").exists()

# Reload clean df & state_cols robustly
df = pd.read_csv(OUT_DIR/"final_traj_clean_training_heart_v2_sorted_imputed.csv", low_memory=False)
with open("/content/feature_config.json") as f:
    feat_cfg = json.load(f)

state_cols = []
for g in ['vitals','labs','recency','flags']:
    if g in feat_cfg:
        state_cols += [c for c in feat_cfg[g] if c in df.columns]

X = df[state_cols].apply(pd.to_numeric, errors='coerce').fillna(0.0)
y = df['action_mapped'].astype(int).values
num_classes = df['action_mapped'].nunique()

base = lgb.LGBMClassifier(
    objective="multiclass", num_class=num_classes,
    learning_rate=0.08, n_estimators=250,
    num_leaves=31, subsample=0.9, colsample_bytree=0.8,
    random_state=42
)
base.fit(X, y)

# Isotonic calibration on full data (cv=3 to be stable)
try:
    cal_iso = CalibratedClassifierCV(base, method="isotonic", cv=3)
except TypeError:
    cal_iso = CalibratedClassifierCV(base_estimator=base, method="isotonic", cv=3)
cal_iso.fit(X, y)

joblib.dump(cal_iso, OUT_DIR/"behavior_model_lgbm_statecols_ISO.pkl")
print("‚úÖ Saved isotonic-calibrated behavior model ->", OUT_DIR/"behavior_model_lgbm_statecols_ISO.pkl")

# Cell D1 ‚Äî policy_deploy_config.json using isotonic-calibrated behavior
import json, pathlib, shutil

OUT = pathlib.Path("/content/crmab_outputs")
SUM = json.loads((OUT/"policy_mix_selected_summary.json").read_text())

deploy_cfg = {
    "policy_type": "mix_softmax_over_q",
    "calibration": "isotonic",
    "temp": float(SUM["temp"]),
    "alpha": float(SUM["alpha"]),
    "tau": float(SUM["tau"]),
    "gamma": float(SUM["gamma"]),
    "clip_ratio": 3.0,
    "paths": {
        "behavior_model": str(OUT/"behavior_model_lgbm_statecols_ISO.pkl"),  # << isotonic
        "q_models": str(OUT/"q_models_fqe_strong.pkl"),
        "action_map": str(OUT/"action_label_map.json"),
        "action_map_inv": str(OUT/"action_label_map_inv.json"),
        "feature_config": "/content/feature_config.json"
    }
}

# copy feature config into OUT for portability
src_feat = pathlib.Path("/content/feature_config.json")
dst_feat = OUT/"feature_config.json"
if src_feat.exists():
    shutil.copy(src_feat, dst_feat)
    deploy_cfg["paths"]["feature_config"] = str(dst_feat)

(OUT/"policy_deploy_config.json").write_text(json.dumps(deploy_cfg, indent=2))
print("‚úÖ Wrote:", OUT/"policy_deploy_config.json")

# Cell D2 ‚Äî build policy_bundle.pkl with isotonic behavior model
import joblib, json, pickle, pathlib

OUT = pathlib.Path("/content/crmab_outputs")
behavior_iso = joblib.load(OUT/"behavior_model_lgbm_statecols_ISO.pkl")
Q_models = joblib.load(OUT/"q_models_fqe_strong.pkl")

with open(OUT/"rmab_sequences.pkl","rb") as f:
    rmab = pickle.load(f)
state_cols = rmab["state_cols"]

SUM = json.loads((OUT/"policy_mix_selected_summary.json").read_text())
bundle = {
    "policy": {
        "type": "mix_softmax_over_q",
        "calibration": "isotonic",
        "temp": float(SUM["temp"]),
        "alpha": float(SUM["alpha"]),
        "tau": float(SUM["tau"]),
        "gamma": float(SUM["gamma"]),
        "clip_ratio": 3.0
    },
    "state_cols": state_cols,
    "models": {
        "behavior": behavior_iso,   # << isotonic
        "q_models": Q_models
    },
    "paths": {
        "action_map": str(OUT/"action_label_map.json"),
        "action_map_inv": str(OUT/"action_label_map_inv.json")
    }
}
joblib.dump(bundle, OUT/"policy_bundle.pkl")
print("‚úÖ Saved:", OUT/"policy_bundle.pkl")

# run in Colab
import os, json, joblib, pickle, pandas as pd, numpy as np
from pathlib import Path

BASE = Path("/content/crmab_outputs")
assert BASE.exists(), BASE

def human(n):
    for u in ['B','KB','MB','GB']:
        if n < 1024: return f"{n:.1f}{u}"
        n /= 1024
    return f"{n:.1f}TB"

print("Files in:", BASE)
for p in sorted(BASE.iterdir()):
    print(f" - {p.name:40s} {human(p.stat().st_size):>8s}")

# Quick head previews for common types
def preview(p):
    if p.suffix.lower() in ['.csv']:
        print("\nCSV head:")
        print(pd.read_csv(p, nrows=5))
    elif p.suffix.lower() in ['.json']:
        print("\nJSON preview:")
        print(json.loads(p.read_text()))
    elif p.suffix.lower() in ['.pkl','.joblib']:
        print("\nPKL preview (keys / type):")
        try:
            obj = joblib.load(p)
            if isinstance(obj, dict):
                print("dict keys:", list(obj.keys())[:50])
            else:
                print("Type:", type(obj))
                # don't print heavy models
        except Exception as e:
            print("Could not joblib.load:", e)
    else:
        print("no preview for", p.name)

# show previews for the files you care about
candidates = [
 "policy_mix_selected_full_ISOTONIC.json",
 "policy_mix_selected_full_ISOTONIC.pkl",
 "policy_bundle.pkl",
 "behavior_model_lgbm_statecols_ISO.pkl",
 "behavior_model_lgbm_statecols.pkl",
 "q_models_fqe_strong.pkl",
 "action_label_map.json",
 "action_label_map_inv.json",
 "final_traj_clean_training_heart_v2_sorted_imputed.csv",
 "feature_config.json"
]
for fname in candidates:
    p = BASE / fname
    if p.exists():
        print("\n=== Preview", fname, "===")
        preview(p)
    else:
        print("\n( missing )", fname)

# safe loader and explorer
import joblib
from pathlib import Path
BASE = Path("/content/crmab_outputs")
p = BASE/"policy_bundle.pkl"
assert p.exists()

bundle = joblib.load(p)   # or pickle.load(open(...,"rb"))
print("Bundle type:", type(bundle))

# Common patterns: bundle might be dict with keys like:
#  { "behavior_model": path_or_model, "q_models": {...}, "state_cols": [...], "policy_config": {...} }
if isinstance(bundle, dict):
    print("Top-level keys:", list(bundle.keys()))
    for k,v in bundle.items():
        if isinstance(v, (list,dict)):
            print(f" - {k}: {type(v)}, len={len(v)}")
        else:
            print(f" - {k}: {type(v)}")
else:
    # fallback: small pkl might contain only config
    print("Bundle content:", str(bundle)[:400])

# single-patient/timestep recommendation helper
import joblib, json, numpy as np, pandas as pd
from pathlib import Path
from scipy.special import softmax

BASE = Path("/content/crmab_outputs")

# 1) load action mapping
action_inv = json.loads((BASE/"action_label_map_inv.json").read_text())
# convert keys to int->orig label (if saved as strings)
action_inv = {int(k): v for k,v in action_inv.items()}

# 2) load bundle (or fallback components)
bundle = joblib.load(BASE/"policy_bundle.pkl")
print("bundle keys:", getattr(bundle, "keys", lambda: None)())

# locate behavior model and q_models
# possible patterns: bundle may contain direct objects or paths
behavior_model = None
q_models = None
state_cols = None
policy_cfg = {}

if isinstance(bundle, dict):
    # try common placements
    behavior_model = bundle.get("behavior_model") or bundle.get("behavior_model_path")
    q_models = bundle.get("q_models") or bundle.get("Q_models") or bundle.get("q_model_paths")
    state_cols = bundle.get("state_cols") or bundle.get("state_columns")
    policy_cfg = bundle.get("policy_cfg") or bundle.get("policy_config") or {}
# if they are paths, load them
if isinstance(behavior_model, str) or (hasattr(behavior_model, "endswith") and behavior_model.endswith(".pkl")):
    behavior_model = joblib.load(BASE/behavior_model)

# q_models might be dict of action->path/object
if isinstance(q_models, dict):
    for a, v in list(q_models.items()):
        if isinstance(v, str):
            q_models[a] = joblib.load(BASE/v)
else:
    # fallback: try loading q_models_fqe_strong.pkl
    if (BASE/"q_models_fqe_strong.pkl").exists():
        q_models = joblib.load(BASE/"q_models_fqe_strong.pkl")

# fallback loads
if behavior_model is None and (BASE/"behavior_model_lgbm_statecols_ISO.pkl").exists():
    behavior_model = joblib.load(BASE/"behavior_model_lgbm_statecols_ISO.pkl")
if state_cols is None and (BASE/"feature_config.json").exists():
    feat = json.loads((BASE/"feature_config.json").read_text())
    # safe extraction
    state_cols = []
    for g in ['vitals','labs','recency','flags']:
        if g in feat:
            state_cols += feat[g]
    state_cols = [c for c in state_cols if isinstance(c, str)]

print("Loaded behavior model:", type(behavior_model))
print("Loaded q_models:", "dict" if isinstance(q_models, dict) else type(q_models))
print("state cols len:", len(state_cols) if state_cols else None)

# 3) recommend function
def recommend_action_for_state(S_row, deterministic=True, temp=0.25, alpha=0.2, tau=0.01):
    """
    S_row: 1D array-like with same order as state_cols
    returns: recommended original action label and action_index
    """
    X = pd.DataFrame([S_row], columns=state_cols).fillna(0.0)
    b = behavior_model.predict_proba(X)[0]   # (K,)
    # get q values
    actions = sorted(list(q_models.keys()))
    q = np.array([q_models[a].predict(X.values)[0] for a in actions])  # (K,)
    pi = softmax(q / temp)
    # support floor: zero out tiny b entries? (following pipeline)
    if tau > 0:
        mask = b < tau
        pi = np.where(mask, 0.0, pi)
        s = pi.sum()
        if s > 0:
            pi = pi / s
        else:
            pi = b  # fallback
    # mixture
    pi_mix = (1-alpha) * b + alpha * pi
    if deterministic:
        ai = int(np.argmax(pi_mix))
    else:
        ai = int(np.random.choice(len(pi_mix), p=pi_mix / pi_mix.sum()))
    return ai, action_inv.get(ai, ai), pi_mix

# Example usage:
# build or fetch a test state vector S_test aligned with state_cols:
# S_test = np.zeros(len(state_cols))   # replace with real features
# ai, orig_label, probs = recommend_action_for_state(S_test, deterministic=True)
# print("action_index", ai, "orig_action", orig_label, "probs", probs)

# Colab-friendly: zip and download
import shutil
shutil.make_archive("/content/crmab_outputs", 'zip', "/content/crmab_outputs")
# if in Colab:
from google.colab import files
files.download("/content/crmab_outputs.zip")